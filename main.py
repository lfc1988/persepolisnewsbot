import requests
from bs4 import BeautifulSoup
import os
import time

# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
BOT_TOKEN = os.getenv("BOT_TOKEN")
CHAT_ID = os.getenv("CHAT_ID")
SENT_LINKS_FILE = "sent_links.txt"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
# Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
KEYWORDS = ["Ù¾Ø±Ø³Ù¾ÙˆÙ„ÛŒØ³", "Ù„Ù†Ú¯"] # Ø¨Ù‡ØªØ± Ø§Ø³Øª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯

def load_sent_links():
    """Ø®ÙˆØ§Ù†Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„Ø§ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯Ù‡ Ø§Ø² ÙØ§ÛŒÙ„."""
    if not os.path.exists(SENT_LINKS_FILE):
        return set()
    with open(SENT_LINKS_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f)

def save_link(link):
    """Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú© Ø¬Ø¯ÛŒØ¯ Ø¯Ø± ÙØ§ÛŒÙ„."""
    with open(SENT_LINKS_FILE, "a", encoding="utf-8") as f:
        f.write(f"{link}\n")

def simple_summary(title, text, max_len=300):
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ (Ø¨Ø±Ø´ Ù…ØªÙ†) Ø¨Ù‡ Ø¬Ø§ÛŒ Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ†."""
    
    # ØªØ±Ú©ÛŒØ¨ ØªÛŒØªØ± Ùˆ Ù…ØªÙ† Ùˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø­Ø¯Ø§Ú©Ø«Ø± 300 Ú©Ø§Ø±Ø§Ú©ØªØ±
    combined_text = f"{title}\n\n{text}"
    if len(combined_text) > max_len:
        return combined_text[:max_len-3] + "..."
    return combined_text

def send_to_telegram(photo_url, caption):
    """Ø§Ø±Ø³Ø§Ù„ Ø¹Ú©Ø³ Ùˆ Ù…ØªÙ† Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…."""
    url = f"https://api.telegram.org/bot{BOT_TOKEN}/sendPhoto"
    
    data = {
        "chat_id": CHAT_ID,
        "photo": photo_url,
        "caption": caption
    }
    try:
        resp = requests.post(url, data=data, timeout=10)
        resp.raise_for_status() # Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø·Ø§Ù‡Ø§ÛŒ HTTP
        print(f"Successfully sent post. Status: {resp.status_code}")
    except requests.exceptions.RequestException as e:
        print(f"Error sending to Telegram: {e}")

# --- ØªÙˆØ§Ø¨Ø¹ Ø§Ø³Ú©Ø±Ù¾ÛŒÙ†Ú¯ ---

def crawl_site(url, item_selector, title_selector, summary_selector, site_name):
    print(f"Checking {site_name}...")
    try:
        page = requests.get(url, headers=HEADERS, timeout=15)
        page.raise_for_status()
        soup = BeautifulSoup(page.text, "html.parser")
        sent_links = load_sent_links()

        news_list = soup.select(item_selector) 
        
        for item in news_list:
            try:
                # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú© Ùˆ Ø¹Ù†ÙˆØ§Ù†
                link_tag = item.select_one("a")
                if not link_tag or not link_tag.get('href'): continue
                
                href = link_tag['href']
                # Ø§ÛŒØ¬Ø§Ø¯ Ù„ÛŒÙ†Ú© Ú©Ø§Ù…Ù„
                if not href.startswith("http"):
                    base_url = "/".join(url.split("/")[:3]) 
                    full_link = f"{base_url}{href}"
                else:
                    full_link = href
                    
                # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯Ù†
                if full_link in sent_links: continue

                title_tag = item.select_one(title_selector)
                title = title_tag.text.strip() if title_tag else "N/A"
                
                # 2. ÙÛŒÙ„ØªØ± Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
                if not any(keyword in title for keyword in KEYWORDS): continue
                
                # 3. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø®Ù„Ø§ØµÙ‡
                summary_tag = item.select_one(summary_selector)
                text = summary_tag.text.strip() if summary_tag else ""
                
                # 4. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ú©Ø³
                img_tag = item.select_one("img")
                photo = img_tag.get('src') if img_tag and img_tag.get('src') else None
                if not photo: continue # Ø§Ú¯Ø± Ø¹Ú©Ø³ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø®Ø¨Ø± Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ù†Ú©Ù†

                # 5. Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ø³Ø§Ø¯Ù‡
                caption = simple_summary(title, text)
                caption_with_link = f"ğŸ”´ {title}\n\n{caption}\n\nğŸ”— Ù…Ù†Ø¨Ø¹: {site_name}"
                
                # 6. Ø§Ø±Ø³Ø§Ù„
                send_to_telegram(photo, caption_with_link)
                save_link(full_link)
                
                time.sleep(1) # ÙˆÙ‚ÙÙ‡ Ú©ÙˆØªØ§Ù‡ Ø¨ÛŒÙ† Ø§Ø±Ø³Ø§Ù„â€ŒÙ‡Ø§

            except Exception as e:
                print(f"Error processing item in {site_name}: {e}")

    except requests.exceptions.RequestException as e:
        print(f"Connection Error for {site_name}: {e}")

def crawl_all():
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„ÛŒÙ†Ú© Ù…Ø³ØªÙ‚ÛŒÙ… ØªÚ¯ Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
    
    # 1. ÙˆØ±Ø²Ø´ 3
    crawl_site(
        url="https://www.varzesh3.com/news/tag/43/%D9%BE%D8%B1%D8%B3%D9%BE%D9%88%D9%84%DB%8C%D8%B3",
        item_selector=".news-main-list li",
        title_selector=".title",
        summary_selector=".summary",
        site_name="ÙˆØ±Ø²Ø´ 3"
    )

    # 2. ÙÙˆØªØ¨Ø§Ù„ 360
    crawl_site(
        url="https://football360.ir/tag/%D9%BE%D8%B1%D8%B3%D9%BE%D9%88%D9%84%DB%8C%D8%B3",
        item_selector=".item.news-list",
        title_selector="h2 a",
        summary_selector=".item-summary",
        site_name="ÙÙˆØªØ¨Ø§Ù„ 360"
    )

    # 3. ÙÙˆØªØ¨Ø§Ù„ÛŒ
    crawl_site(
        url="https://www.fotballi.net/tag/%D9%BE%D8%B1%D8%B3%D9%BE%D9%88%D9%84%DB%8C%D8%B3",
        item_selector=".list-item-content",
        title_selector=".item-title a",
        summary_selector=".item-description",
        site_name="ÙÙˆØªØ¨Ø§Ù„ÛŒ"
    )

if __name__ == "__main__":
    crawl_all()
